{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Health Registry Data Profiling and Cleaning\n",
        "\n",
        "This notebook contains exploratory data analysis (EDA) and a reproducible cleaning pipeline for the National Health Facility Registry dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the raw data\n",
        "df_raw = pd.read_csv('health_registry.csv')\n",
        "\n",
        "print(f\"Dataset shape: {df_raw.shape}\")\n",
        "print(f\"\\nColumns: {list(df_raw.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df_raw.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Exploratory Data Analysis\n",
        "\n",
        "### 2.1 Data Overview\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic information\n",
        "print(\"Dataset Info:\")\n",
        "print(f\"Shape: {df_raw.shape[0]} rows × {df_raw.shape[1]} columns\")\n",
        "print(f\"\\nMemory usage: {df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "print(f\"\\nData types:\\n{df_raw.dtypes}\")\n",
        "print(f\"\\nNon-null counts:\\n{df_raw.count()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Missingness Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count missing values (including empty strings and 'NULL' strings)\n",
        "missing_analysis = pd.DataFrame({\n",
        "    'Total_Rows': len(df_raw),\n",
        "    'Null_Count': df_raw.isnull().sum(),\n",
        "    'Empty_String_Count': (df_raw == '').sum(),\n",
        "    'NULL_String_Count': (df_raw == 'NULL').sum(),\n",
        "    'N/A_Count': (df_raw == 'N/A').sum()\n",
        "})\n",
        "\n",
        "missing_analysis['Total_Missing'] = (\n",
        "    missing_analysis['Null_Count'] + \n",
        "    missing_analysis['Empty_String_Count'] + \n",
        "    missing_analysis['NULL_String_Count']\n",
        ")\n",
        "missing_analysis['Missing_Percentage'] = (\n",
        "    missing_analysis['Total_Missing'] / missing_analysis['Total_Rows'] * 100\n",
        ")\n",
        "\n",
        "print(\"Missingness Analysis:\")\n",
        "print(missing_analysis.round(2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exact duplicates\n",
        "exact_duplicates = df_raw.duplicated(keep=False)\n",
        "exact_dup_count = exact_duplicates.sum()\n",
        "\n",
        "print(f\"Exact duplicate rows: {exact_dup_count}\")\n",
        "print(f\"Unique duplicate groups: {df_raw[exact_duplicates].drop_duplicates().shape[0]}\")\n",
        "\n",
        "# Sample of exact duplicates\n",
        "if exact_dup_count > 0:\n",
        "    print(\"\\nSample of exact duplicates:\")\n",
        "    df_raw[exact_duplicates].head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Data Quality Issues by Column\n",
        "\n",
        "Let's examine each column for quality issues:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Facility ID analysis\n",
        "print(\"=== Facility ID Analysis ===\")\n",
        "print(f\"Unique values: {df_raw['facility_id'].nunique()}\")\n",
        "print(f\"\\nSample values:\\n{df_raw['facility_id'].value_counts().head(20)}\")\n",
        "\n",
        "# Check for patterns\n",
        "print(f\"\\nNull/empty/NULL: {(df_raw['facility_id'].isna() | (df_raw['facility_id'] == '') | (df_raw['facility_id'] == 'NULL')).sum()}\")\n",
        "print(f\"\\nValues starting with '#': {df_raw['facility_id'].str.startswith('#', na=False).sum()}\")\n",
        "print(f\"Values starting with 'HF-': {df_raw['facility_id'].str.startswith('HF-', na=False).sum()}\")\n",
        "print(f\"Hex-like values (6 char alphanumeric): {df_raw['facility_id'].str.match(r'^[0-9a-f]{6}$', na=False).sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Facility Name analysis\n",
        "print(\"=== Facility Name Analysis ===\")\n",
        "print(f\"Unique values: {df_raw['facility_name'].nunique()}\")\n",
        "\n",
        "# Check for emojis and special characters\n",
        "sample_names = df_raw['facility_name'].dropna().head(50)\n",
        "print(\"\\nSample facility names with issues:\")\n",
        "for name in sample_names:\n",
        "    if any(ord(char) > 127 for char in str(name)) or '\\n' in str(name):\n",
        "        print(f\"  - {repr(name)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Facility Type analysis\n",
        "print(\"=== Facility Type Analysis ===\")\n",
        "print(f\"Unique values: {df_raw['facility_type'].nunique()}\")\n",
        "print(f\"\\nValue counts:\\n{df_raw['facility_type'].value_counts()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Capacity analysis\n",
        "print(\"=== Capacity Analysis ===\")\n",
        "print(f\"Unique values: {df_raw['capacity'].nunique()}\")\n",
        "print(f\"\\nSample values:\\n{df_raw['capacity'].value_counts().head(30)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Region analysis\n",
        "print(\"=== Region Analysis ===\")\n",
        "print(f\"Unique values: {df_raw['region'].nunique()}\")\n",
        "print(f\"\\nValue counts:\\n{df_raw['region'].value_counts().head(30)}\")\n",
        "\n",
        "# Check for reversed text patterns\n",
        "reversed_pattern = df_raw['region'].str.contains(r'\\.tS', na=False, case=False)\n",
        "print(f\"\\nPotential reversed text (contains '.tS'): {reversed_pattern.sum()}\")\n",
        "if reversed_pattern.sum() > 0:\n",
        "    print(\"\\nSample reversed regions:\")\n",
        "    print(df_raw[reversed_pattern]['region'].unique()[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Date analysis\n",
        "print(\"=== Date Format Analysis ===\")\n",
        "print(\"Licence Issue Date sample values:\")\n",
        "print(df_raw['licence_issue_date'].value_counts().head(20))\n",
        "print(\"\\nInspection Date sample values:\")\n",
        "print(df_raw['inspection_date'].value_counts().head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPS Location analysis\n",
        "print(\"=== GPS Location Analysis ===\")\n",
        "print(f\"Unique formats detected:\")\n",
        "\n",
        "gps_sample = df_raw['gps_location'].dropna().head(100)\n",
        "print(\"\\nSample GPS formats:\")\n",
        "for gps in gps_sample.unique()[:20]:\n",
        "    print(f\"  - {repr(gps)}\")\n",
        "\n",
        "# Count format types\n",
        "point_format = df_raw['gps_location'].str.contains('POINT', na=False).sum()\n",
        "degree_format = df_raw['gps_location'].str.contains('°', na=False).sum()\n",
        "comma_format = df_raw['gps_location'].str.contains(',', na=False).sum() - point_format\n",
        "semicolon_format = df_raw['gps_location'].str.contains(';', na=False).sum()\n",
        "\n",
        "print(f\"\\nFormat counts:\")\n",
        "print(f\"  POINT(): {point_format}\")\n",
        "print(f\"  Degree-minute-second: {degree_format}\")\n",
        "print(f\"  Comma-separated: {comma_format}\")\n",
        "print(f\"  Semicolon-separated: {semicolon_format}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Cleaning Pipeline\n",
        "\n",
        "Now we'll implement a step-by-step cleaning pipeline:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy for cleaning\n",
        "df_clean = df_raw.copy()\n",
        "initial_rows = len(df_clean)\n",
        "print(f\"Starting with {initial_rows} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Remove test/invalid rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify invalid rows (containing test data patterns)\n",
        "invalid_patterns = [\n",
        "    r'^\\?\\?\\?',  # Starts with ???\n",
        "    r'not a date',  # Invalid date text\n",
        "    r'^\\s*$'  # Completely empty rows\n",
        "]\n",
        "\n",
        "# Check for rows with invalid patterns in key columns\n",
        "invalid_mask = pd.Series([False] * len(df_clean))\n",
        "\n",
        "for col in ['facility_id', 'facility_name', 'licence_issue_date', 'inspection_date']:\n",
        "    for pattern in invalid_patterns:\n",
        "        invalid_mask |= df_clean[col].str.contains(pattern, na=False, regex=True)\n",
        "\n",
        "# Also check for rows where all important columns are empty/null\n",
        "key_cols = ['facility_id', 'facility_name', 'region']\n",
        "invalid_mask |= df_clean[key_cols].isnull().all(axis=1) | (df_clean[key_cols] == '').all(axis=1)\n",
        "\n",
        "invalid_count = invalid_mask.sum()\n",
        "print(f\"Removing {invalid_count} invalid/test rows\")\n",
        "df_clean = df_clean[~invalid_mask].copy()\n",
        "print(f\"Rows remaining: {len(df_clean)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Standardize facility_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_facility_id(facility_id):\n",
        "    \"\"\"Clean facility ID: extract only numbers and convert to int\"\"\"\n",
        "    if pd.isna(facility_id) or facility_id == '' or str(facility_id).strip() == 'NULL':\n",
        "        return None\n",
        "    \n",
        "    id_str = str(facility_id).strip()\n",
        "    \n",
        "    # Remove leading #\n",
        "    if id_str.startswith('#'):\n",
        "        id_str = id_str[1:]\n",
        "    \n",
        "    # Extract only digits\n",
        "    digits = re.sub(r'[^0-9]', '', id_str)\n",
        "    \n",
        "    if digits == '':\n",
        "        return None\n",
        "    \n",
        "    # Convert to int\n",
        "    try:\n",
        "        return int(digits)\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "df_clean['facility_id_clean'] = df_clean['facility_id'].apply(clean_facility_id)\n",
        "\n",
        "# For deduplication, create a normalized ID (for comparison only)\n",
        "def normalize_id_for_dedup(fid):\n",
        "    \"\"\"Create normalized ID for duplicate detection\"\"\"\n",
        "    if pd.isna(fid):\n",
        "        return None\n",
        "    # Since IDs are now int, just convert to string for comparison\n",
        "    return str(int(fid)) if pd.notna(fid) else None\n",
        "\n",
        "df_clean['facility_id_normalized'] = df_clean['facility_id_clean'].apply(normalize_id_for_dedup)\n",
        "\n",
        "print(f\"Facility IDs cleaned (extracted numbers, converted to int). Sample:\")\n",
        "print(df_clean[['facility_id', 'facility_id_clean']].head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reverse_region_name(region_text):\n",
        "    \"\"\"Reverse a region name that's been written backwards\"\"\"\n",
        "    # Mapping of reversed region names to correct ones\n",
        "    reversed_regions = {\n",
        "        'semaj': 'St. James',\n",
        "        'nhoj': 'St. John',\n",
        "        'retep': 'St. Peter',\n",
        "        'werdna': 'St. Andrew',\n",
        "        'leahcim': 'St. Michael',\n",
        "        'hpesoj': 'St. Joseph',\n",
        "        'egroeg': 'St. George',\n",
        "        'hcruhc tsirhc': 'Christ Church',\n",
        "        'tsirhc hcruhc': 'Christ Church',\n",
        "    }\n",
        "    \n",
        "    region_lower = region_text.lower().strip()\n",
        "    \n",
        "    # Try exact match first\n",
        "    if region_lower in reversed_regions:\n",
        "        return reversed_regions[region_lower]\n",
        "    \n",
        "    # Try partial match (e.g., 'semaj' in '(semaj)')\n",
        "    for reversed_key, correct_name in reversed_regions.items():\n",
        "        if reversed_key in region_lower:\n",
        "            return correct_name\n",
        "    \n",
        "    return None\n",
        "\n",
        "def clean_facility_name(name):\n",
        "    \"\"\"Clean facility name: fix abbreviations, reverse prefixes, move suffixes, remove special chars\"\"\"\n",
        "    if pd.isna(name) or name == '':\n",
        "        return None\n",
        "    \n",
        "    name_str = str(name)\n",
        "    \n",
        "    # Remove newlines and normalize whitespace\n",
        "    cleaned = re.sub(r'\\s+', ' ', name_str)\n",
        "    cleaned = cleaned.replace('\\n', ' ').replace('\\r', ' ')\n",
        "    \n",
        "    # Handle backwards prefixes in parentheses: (Semaj) -> St. James, (nhoj) -> St. John\n",
        "    # Find patterns like (semaj), (nhoj), etc.\n",
        "    paren_pattern = r'\\(([^)]+)\\)'\n",
        "    matches = list(re.finditer(paren_pattern, cleaned, re.IGNORECASE))\n",
        "    \n",
        "    # Process matches in reverse order to maintain indices\n",
        "    replacements = []\n",
        "    for match in reversed(matches):\n",
        "        content = match.group(1).strip()\n",
        "        # Check if it's a backwards region name\n",
        "        reversed_region = reverse_region_name(content)\n",
        "        if reversed_region:\n",
        "            replacements.append((match.start(), match.end(), reversed_region))\n",
        "        # Check if it's a suffix like (St.), (st.), (ST.)\n",
        "        elif re.match(r'^[Ss]t\\.?\\s*$', content):\n",
        "            # Move suffix to front\n",
        "            suffix = content.strip()\n",
        "            cleaned = cleaned[:match.start()] + cleaned[match.end():]\n",
        "            cleaned = f\"{suffix} {cleaned}\".strip()\n",
        "    \n",
        "    # Apply region replacements\n",
        "    for start, end, replacement in replacements:\n",
        "        cleaned = cleaned[:start] + replacement + cleaned[end:]\n",
        "    \n",
        "    # Fix common abbreviations: Hosp. -> Hospital, Clin. -> Clinic\n",
        "    cleaned = re.sub(r'\\bHosp\\.\\b', 'Hospital', cleaned)\n",
        "    cleaned = re.sub(r'\\bClin\\.\\b', 'Clinic', cleaned)\n",
        "    \n",
        "    # Remove emojis and special unicode characters first\n",
        "    cleaned = ''.join(char for char in cleaned if ord(char) < 128)\n",
        "    \n",
        "    # Remove all special characters (keep only alphanumeric, spaces, commas, periods, hyphens, apostrophes)\n",
        "    cleaned = re.sub(r'[^a-zA-Z0-9\\s,\\.\\-\\']', '', cleaned)\n",
        "    \n",
        "    # Normalize whitespace again\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
        "    \n",
        "    return cleaned if cleaned else None\n",
        "\n",
        "df_clean['facility_name_clean'] = df_clean['facility_name'].apply(clean_facility_name)\n",
        "\n",
        "print(\"Sample of cleaned facility names:\")\n",
        "comparison = pd.DataFrame({\n",
        "    'Original': df_clean['facility_name'].head(20),\n",
        "    'Cleaned': df_clean['facility_name_clean'].head(20)\n",
        "})\n",
        "print(comparison[comparison['Original'] != comparison['Cleaned']].head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Standardize facility_type\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mapping of variants to standard types\n",
        "facility_type_mapping = {\n",
        "    'hosp.': 'Hospital',\n",
        "    'hospital': 'Hospital',\n",
        "    'health ctr.': 'Health Centre',\n",
        "    'health centre': 'Health Centre',\n",
        "    'health center': 'Health Centre',\n",
        "    'community health ctr.': 'Community Health Centre',\n",
        "    'community health centre': 'Community Health Centre',\n",
        "    'community health center': 'Community Health Centre',\n",
        "    'chc': 'Community Health Centre',\n",
        "    'polyclinic': 'Polyclinic',\n",
        "    'clinic': 'Clinic',\n",
        "    'health centre': 'Health Centre',\n",
        "    'infirmary': 'Infirmary'\n",
        "}\n",
        "\n",
        "def standardize_facility_type(fac_type):\n",
        "    \"\"\"Standardize facility type\"\"\"\n",
        "    if pd.isna(fac_type) or fac_type == '':\n",
        "        return None\n",
        "    \n",
        "    fac_type_str = str(fac_type).strip()\n",
        "    \n",
        "    # Try to match (case-insensitive)\n",
        "    fac_type_lower = fac_type_str.lower()\n",
        "    \n",
        "    # Check for exact matches first\n",
        "    for key, value in facility_type_mapping.items():\n",
        "        if key in fac_type_lower:\n",
        "            return value\n",
        "    \n",
        "    # If no match, try to clean and return\n",
        "    # Remove emojis and special chars\n",
        "    cleaned = ''.join(char for char in fac_type_str if ord(char) < 128)\n",
        "    return cleaned.strip() if cleaned.strip() else None\n",
        "\n",
        "df_clean['facility_type_clean'] = df_clean['facility_type'].apply(standardize_facility_type)\n",
        "\n",
        "print(\"Facility type standardization:\")\n",
        "print(f\"\\nBefore: {df_clean['facility_type'].value_counts().head(15)}\")\n",
        "print(f\"\\nAfter: {df_clean['facility_type_clean'].value_counts()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Parse capacity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_capacity(capacity):\n",
        "    \"\"\"Parse capacity to numeric value\"\"\"\n",
        "    if pd.isna(capacity) or capacity == '':\n",
        "        return None, None\n",
        "    \n",
        "    cap_str = str(capacity).strip()\n",
        "    \n",
        "    # Handle N/A and unknown\n",
        "    if cap_str.upper() in ['N/A', 'NAN', 'UNKNOWN', 'NONE']:\n",
        "        return None, None\n",
        "    \n",
        "    # Handle \"ten beds\" pattern\n",
        "    if 'ten' in cap_str.lower():\n",
        "        return 10, 'beds'\n",
        "    \n",
        "    # Extract number\n",
        "    number_match = re.search(r'(\\d+)', cap_str)\n",
        "    if number_match:\n",
        "        number = int(number_match.group(1))\n",
        "        \n",
        "        # Determine unit\n",
        "        cap_lower = cap_str.lower()\n",
        "        if 'bed' in cap_lower:\n",
        "            unit = 'beds'\n",
        "        elif 'cot' in cap_lower:\n",
        "            unit = 'cots'\n",
        "        elif 'patient' in cap_lower:\n",
        "            unit = 'patients'\n",
        "        elif 'capacity' in cap_lower:\n",
        "            unit = 'capacity'\n",
        "        else:\n",
        "            unit = 'units'  # default\n",
        "        \n",
        "        return number, unit\n",
        "    \n",
        "    return None, None\n",
        "\n",
        "capacity_results = df_clean['capacity'].apply(parse_capacity)\n",
        "df_clean['capacity_numeric'] = [r[0] for r in capacity_results]\n",
        "df_clean['capacity_unit'] = [r[1] for r in capacity_results]\n",
        "\n",
        "print(\"Capacity parsing summary:\")\n",
        "print(f\"Successfully parsed: {df_clean['capacity_numeric'].notna().sum()}\")\n",
        "print(f\"\\nSample parsed capacities:\")\n",
        "print(df_clean[['capacity', 'capacity_numeric', 'capacity_unit']].head(30))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reverse_text(text):\n",
        "    \"\"\"Reverse a text string\"\"\"\n",
        "    return text[::-1]\n",
        "\n",
        "def normalize_region(region):\n",
        "    \"\"\"Normalize region name, handling backwards text\"\"\"\n",
        "    if pd.isna(region) or region == '':\n",
        "        return None\n",
        "    \n",
        "    region_str = str(region).strip()\n",
        "    region_lower = region_str.lower()\n",
        "    \n",
        "    # Mapping of known regions (Barbados parishes) including reversed variants\n",
        "    canonical_regions = {\n",
        "        'st. james': 'St. James', 'st james': 'St. James', 'st.james': 'St. James',\n",
        "        'st. james parish': 'St. James',\n",
        "        'semaj .ts': 'St. James', 'semaj .tS': 'St. James', 'semaj': 'St. James',\n",
        "        'retep .ts': 'St. Peter', 'retep': 'St. Peter',\n",
        "        'st. peter': 'St. Peter', 'st peter': 'St. Peter', 'st.peter': 'St. Peter',\n",
        "        'st. peter parish': 'St. Peter',\n",
        "        'werdna .ts': 'St. Andrew', 'werdna .tS': 'St. Andrew', 'werdna': 'St. Andrew',\n",
        "        'st. andrew': 'St. Andrew', 'st andrew': 'St. Andrew', 'st.andrew': 'St. Andrew',\n",
        "        'st. andrew parish': 'St. Andrew',\n",
        "        'leahcim .ts': 'St. Michael', 'leahcim': 'St. Michael',\n",
        "        'st. michael': 'St. Michael', 'st michael': 'St. Michael', 'st.michael': 'St. Michael',\n",
        "        'st. michael parish': 'St. Michael',\n",
        "        'hpesoj .ts': 'St. Joseph', 'hpesoj': 'St. Joseph', 'nhoj .ts': 'St. John', 'nhoj': 'St. John',\n",
        "        'st. joseph': 'St. Joseph', 'st joseph': 'St. Joseph', 'st.joseph': 'St. Joseph',\n",
        "        'st. joseph parish': 'St. Joseph',\n",
        "        'st. john': 'St. John', 'st john': 'St. John', 'st.john': 'St. John',\n",
        "        'st. john parish': 'St. John',\n",
        "        'egroeg .ts': 'St. George', 'egroeg': 'St. George',\n",
        "        'st. george': 'St. George', 'st george': 'St. George', 'st.george': 'St. George',\n",
        "        'st. george parish': 'St. George',\n",
        "        'hcruhc tsirhc': 'Christ Church', 'hcruhC tsirhC': 'Christ Church',\n",
        "        'christ church': 'Christ Church', 'christchurch': 'Christ Church',\n",
        "        'christ church parish': 'Christ Church',\n",
        "        'st. lucy': 'St. Lucy', 'st lucy': 'St. Lucy', 'st.lucy': 'St. Lucy',\n",
        "        'st. lucy parish': 'St. Lucy',\n",
        "    }\n",
        "    \n",
        "    # Check direct mapping\n",
        "    if region_lower in canonical_regions:\n",
        "        return canonical_regions[region_lower]\n",
        "    \n",
        "    # Try case-insensitive matching\n",
        "    for key, value in canonical_regions.items():\n",
        "        if key in region_lower:\n",
        "            return value\n",
        "    \n",
        "    # Try reversing the text and checking\n",
        "    reversed_text = reverse_text(region_str)\n",
        "    reversed_lower = reversed_text.lower().strip()\n",
        "    \n",
        "    # Check if reversed text matches a known region\n",
        "    for key, value in canonical_regions.items():\n",
        "        if key in reversed_lower or reversed_lower in key:\n",
        "            return value\n",
        "    \n",
        "    # If still no match, try to standardize case\n",
        "    words = region_str.split()\n",
        "    normalized = ' '.join(word.capitalize() if word.lower() not in ['st', 'st.', 'and'] else word.title() \n",
        "                         for word in words)\n",
        "    \n",
        "    return normalized\n",
        "\n",
        "df_clean['region_clean'] = df_clean['region'].apply(normalize_region)\n",
        "\n",
        "print(\"Region normalization summary:\")\n",
        "print(f\"\\nBefore (unique): {df_clean['region'].nunique()}\")\n",
        "print(f\"\\nAfter (unique): {df_clean['region_clean'].nunique()}\")\n",
        "print(f\"\\nCleaned regions:\\n{df_clean['region_clean'].value_counts()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_date(date_str):\n",
        "    \"\"\"Parse various date formats to datetime\"\"\"\n",
        "    if pd.isna(date_str) or date_str == '':\n",
        "        return None\n",
        "    \n",
        "    date_str = str(date_str).strip()\n",
        "    \n",
        "    # List of date formats to try\n",
        "    date_formats = [\n",
        "        '%Y-%m-%d',           # 2016-02-18\n",
        "        '%d-%m-%y',           # 04-01-21\n",
        "        '%d-%m-%Y',           # 04-01-2021\n",
        "        '%d/%m/%y',           # 18/05/18\n",
        "        '%d/%m/%Y',           # 18/05/2018\n",
        "        '%Y%m%d',             # 20160217\n",
        "        '%d %b %Y',           # 08 Mar 2024\n",
        "        '%d %B %Y',           # March 08 2024\n",
        "        '%B %d %Y',           # November 21 2020\n",
        "        '%b %d %Y',           # Mar 28 2024\n",
        "        '%d %B %y',           # March 08 24\n",
        "        '%Y%m%d',             # 20230822\n",
        "        '%y%m%d',             # 210617\n",
        "    ]\n",
        "    \n",
        "    for fmt in date_formats:\n",
        "        try:\n",
        "            return pd.to_datetime(date_str, format=fmt, errors='raise')\n",
        "        except (ValueError, TypeError):\n",
        "            continue\n",
        "    \n",
        "    # Try pandas flexible parsing as last resort\n",
        "    try:\n",
        "        return pd.to_datetime(date_str, errors='raise', infer_datetime_format=True)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "df_clean['licence_issue_date_clean'] = df_clean['licence_issue_date'].apply(parse_date)\n",
        "df_clean['inspection_date_clean'] = df_clean['inspection_date'].apply(parse_date)\n",
        "\n",
        "print(\"Date parsing summary:\")\n",
        "print(f\"Licence dates parsed: {df_clean['licence_issue_date_clean'].notna().sum()} / {len(df_clean)}\")\n",
        "print(f\"Inspection dates parsed: {df_clean['inspection_date_clean'].notna().sum()} / {len(df_clean)}\")\n",
        "\n",
        "# Flag date anomalies\n",
        "df_clean['date_anomaly'] = False\n",
        "df_clean.loc[\n",
        "    (df_clean['inspection_date_clean'].notna()) & \n",
        "    (df_clean['licence_issue_date_clean'].notna()) &\n",
        "    (df_clean['inspection_date_clean'] < df_clean['licence_issue_date_clean']),\n",
        "    'date_anomaly'\n",
        "] = True\n",
        "\n",
        "print(f\"\\nDate anomalies (inspection before licence): {df_clean['date_anomaly'].sum()}\")\n",
        "\n",
        "# Check for future dates (assuming current date is 2024-12-31)\n",
        "current_date = pd.Timestamp('2024-12-31')\n",
        "future_licences = (df_clean['licence_issue_date_clean'] > current_date).sum()\n",
        "future_inspections = (df_clean['inspection_date_clean'] > current_date).sum()\n",
        "print(f\"Future licence dates: {future_licences}\")\n",
        "print(f\"Future inspection dates: {future_inspections}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_gps_coordinates(gps_str):\n",
        "    \"\"\"Parse various GPS formats to decimal degrees (lat, lon)\"\"\"\n",
        "    if pd.isna(gps_str) or gps_str == '' or str(gps_str).upper() == 'N/A':\n",
        "        return None, None\n",
        "    \n",
        "    gps_str = str(gps_str).strip()\n",
        "    \n",
        "    # Handle POINT format: POINT(-58.84001 12.87196)\n",
        "    point_match = re.search(r'POINT\\(([-\\d.]+)\\s+([-\\d.]+)\\)', gps_str, re.IGNORECASE)\n",
        "    if point_match:\n",
        "        lon = float(point_match.group(1))\n",
        "        lat = float(point_match.group(2))\n",
        "        return lat, lon\n",
        "    \n",
        "    # Handle comma-separated: \"13.08576, -58.75331\" or \"-58.82307, 13.00952\"\n",
        "    comma_match = re.search(r'([-\\d.]+)\\s*,\\s*([-\\d.]+)', gps_str)\n",
        "    if comma_match:\n",
        "        val1 = float(comma_match.group(1))\n",
        "        val2 = float(comma_match.group(2))\n",
        "        # Determine which is lat and which is lon based on ranges\n",
        "        # Latitude: -90 to 90, Longitude: -180 to 180\n",
        "        # For Barbados: lat ~12-13, lon ~-59 to -58\n",
        "        if -90 <= val1 <= 90 and -180 <= val2 <= 180:\n",
        "            # First is likely lat, second is lon\n",
        "            if abs(val1) > abs(val2) and val1 > 0:  # If first is clearly lat range\n",
        "                return val1, val2\n",
        "            elif abs(val2) < abs(val1):  # Second is likely lat (smaller absolute value for Barbados)\n",
        "                return val2, val1\n",
        "            else:\n",
        "                return val1, val2\n",
        "        else:\n",
        "            # Assume first is lat, second is lon\n",
        "            return val1, val2\n",
        "    \n",
        "    # Handle semicolon-separated: -58.71602;12.98428\n",
        "    semicolon_match = re.search(r'([-\\d.]+)\\s*;\\s*([-\\d.]+)', gps_str)\n",
        "    if semicolon_match:\n",
        "        val1 = float(semicolon_match.group(1))\n",
        "        val2 = float(semicolon_match.group(2))\n",
        "        # Similar logic to comma\n",
        "        if abs(val1) < abs(val2) and val1 > 0:  # First is likely lat\n",
        "            return val1, val2\n",
        "        else:\n",
        "            return val2, val1\n",
        "    \n",
        "    # Handle degree-minute-second: 13°12′43″N 58°51′50″W\n",
        "    dms_match = re.search(r'(\\d+)°(\\d+)′(\\d+)″([NS])\\s+(\\d+)°(\\d+)′(\\d+)″([EW])', gps_str)\n",
        "    if dms_match:\n",
        "        lat_deg = int(dms_match.group(1))\n",
        "        lat_min = int(dms_match.group(2))\n",
        "        lat_sec = int(dms_match.group(3))\n",
        "        lat_dir = dms_match.group(4)\n",
        "        \n",
        "        lon_deg = int(dms_match.group(5))\n",
        "        lon_min = int(dms_match.group(6))\n",
        "        lon_sec = int(dms_match.group(7))\n",
        "        lon_dir = dms_match.group(8)\n",
        "        \n",
        "        lat = lat_deg + lat_min/60 + lat_sec/3600\n",
        "        if lat_dir == 'S':\n",
        "            lat = -lat\n",
        "        \n",
        "        lon = lon_deg + lon_min/60 + lon_sec/3600\n",
        "        if lon_dir == 'W':\n",
        "            lon = -lon\n",
        "        \n",
        "        return lat, lon\n",
        "    \n",
        "    return None, None\n",
        "\n",
        "gps_results = df_clean['gps_location'].apply(parse_gps_coordinates)\n",
        "df_clean['latitude'] = [r[0] for r in gps_results]\n",
        "df_clean['longitude'] = [r[1] for r in gps_results]\n",
        "\n",
        "print(\"GPS parsing summary:\")\n",
        "print(f\"Successfully parsed: {df_clean['latitude'].notna().sum()} / {len(df_clean)}\")\n",
        "\n",
        "# Validate coordinate ranges (Barbados approximate bounds)\n",
        "valid_coords = (\n",
        "    (df_clean['latitude'] >= 12.5) & (df_clean['latitude'] <= 13.5) &\n",
        "    (df_clean['longitude'] >= -59.7) & (df_clean['longitude'] <= -58.5)\n",
        ") | df_clean['latitude'].isna()\n",
        "\n",
        "print(f\"Coordinates within Barbados bounds: {valid_coords.sum()} / {len(df_clean)}\")\n",
        "print(f\"Out-of-range coordinates: {(~valid_coords).sum()}\")\n",
        "\n",
        "# Set out-of-range to None\n",
        "df_clean.loc[~valid_coords, 'latitude'] = None\n",
        "df_clean.loc[~valid_coords, 'longitude'] = None\n",
        "\n",
        "print(f\"\\nFinal valid coordinates: {df_clean['latitude'].notna().sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 9: Handle duplicates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove exact duplicates first\n",
        "before_exact_dup = len(df_clean)\n",
        "df_clean = df_clean.drop_duplicates(subset=df_clean.columns.tolist(), keep='first')\n",
        "exact_dup_removed = before_exact_dup - len(df_clean)\n",
        "print(f\"Exact duplicates removed: {exact_dup_removed}\")\n",
        "\n",
        "# Handle near-duplicates (same facility name but different IDs)\n",
        "# Create a function to identify potential duplicates based on name similarity\n",
        "def create_name_key(name):\n",
        "    \"\"\"Create a normalized key for name comparison\"\"\"\n",
        "    if pd.isna(name):\n",
        "        return None\n",
        "    # Remove special chars, lowercase, remove common suffixes\n",
        "    name_key = str(name).lower()\n",
        "    name_key = re.sub(r'[^\\w\\s]', '', name_key)\n",
        "    name_key = re.sub(r'\\s+', ' ', name_key).strip()\n",
        "    # Remove common abbreviations\n",
        "    name_key = name_key.replace('st.', 'st').replace('st ', 'st')\n",
        "    name_key = name_key.replace('hosp.', 'hospital').replace('hosp ', 'hospital')\n",
        "    name_key = name_key.replace('clin.', 'clinic').replace('clin ', 'clinic')\n",
        "    return name_key\n",
        "\n",
        "df_clean['name_key'] = df_clean['facility_name_clean'].apply(create_name_key)\n",
        "\n",
        "# Find potential duplicates (same name key and region)\n",
        "potential_dups = df_clean.groupby(['name_key', 'region_clean']).size()\n",
        "potential_dups = potential_dups[potential_dups > 1].reset_index(name='dup_count')\n",
        "\n",
        "print(f\"\\nPotential duplicate groups (same name + region): {len(potential_dups)}\")\n",
        "\n",
        "# For each duplicate group, keep the record with most complete data\n",
        "def completeness_score(row):\n",
        "    \"\"\"Calculate completeness score (higher = more complete)\"\"\"\n",
        "    score = 0\n",
        "    if pd.notna(row.get('facility_name_clean')): score += 1\n",
        "    if pd.notna(row.get('facility_type_clean')): score += 1\n",
        "    if pd.notna(row.get('capacity_numeric')): score += 1\n",
        "    if pd.notna(row.get('licence_issue_date_clean')): score += 1\n",
        "    if pd.notna(row.get('inspection_date_clean')): score += 1\n",
        "    if pd.notna(row.get('latitude')): score += 1\n",
        "    if pd.notna(row.get('remarks')) and str(row.get('remarks')).strip() not in ['', '-', 'N/A']: score += 1\n",
        "    return score\n",
        "\n",
        "df_clean['completeness_score'] = df_clean.apply(completeness_score, axis=1)\n",
        "\n",
        "# Remove duplicates keeping the most complete record\n",
        "df_clean = df_clean.sort_values('completeness_score', ascending=False)\n",
        "df_clean = df_clean.drop_duplicates(subset=['name_key', 'region_clean'], keep='first')\n",
        "\n",
        "print(f\"Rows after duplicate removal: {len(df_clean)}\")\n",
        "df_clean = df_clean.drop(columns=['name_key', 'completeness_score'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 10: Create final cleaned dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean remarks - remove strange characters\n",
        "def clean_remarks(remark):\n",
        "    \"\"\"Clean remarks by removing special characters and emojis\"\"\"\n",
        "    if pd.isna(remark) or remark == '':\n",
        "        return None\n",
        "    \n",
        "    remark_str = str(remark).strip()\n",
        "    \n",
        "    # Remove emojis and special unicode characters\n",
        "    cleaned = ''.join(char for char in remark_str if ord(char) < 128)\n",
        "    \n",
        "    # Remove characters except alphanumeric, spaces, hyphens, commas, periods\n",
        "    cleaned = re.sub(r'[^a-zA-Z0-9\\s\\-,\\.]', '', cleaned)\n",
        "    \n",
        "    # Normalize whitespace\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
        "    \n",
        "    # Handle common empty indicators\n",
        "    if cleaned in ['', '-', 'N/A', 'NULL']:\n",
        "        return None\n",
        "    \n",
        "    return cleaned if cleaned else None\n",
        "\n",
        "df_clean['remarks_clean'] = df_clean['remarks'].apply(clean_remarks)\n",
        "\n",
        "# Create final dataset with cleaned columns\n",
        "df_final = pd.DataFrame({\n",
        "    'facility_id': df_clean['facility_id_clean'],\n",
        "    'facility_name': df_clean['facility_name_clean'],\n",
        "    'facility_type': df_clean['facility_type_clean'],\n",
        "    'capacity_numeric': df_clean['capacity_numeric'],\n",
        "    'capacity_unit': df_clean['capacity_unit'],\n",
        "    'region': df_clean['region_clean'],\n",
        "    'licence_issue_date': df_clean['licence_issue_date_clean'],\n",
        "    'inspection_date': df_clean['inspection_date_clean'],\n",
        "    'latitude': df_clean['latitude'],\n",
        "    'longitude': df_clean['longitude'],\n",
        "    'remarks': df_clean['remarks_clean'],  # Use cleaned remarks\n",
        "    'date_anomaly': df_clean['date_anomaly']  # Flag for date issues\n",
        "})\n",
        "\n",
        "# Replace empty strings with NaN\n",
        "df_final = df_final.replace(['', 'NULL', 'N/A', '-'], np.nan)\n",
        "\n",
        "print(\"Final dataset shape:\", df_final.shape)\n",
        "print(\"\\nData types:\")\n",
        "print(df_final.dtypes)\n",
        "print(\"\\nMissing values:\")\n",
        "print(df_final.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 11: Final validation and summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== CLEANING SUMMARY ===\")\n",
        "print(f\"\\nInitial rows: {initial_rows}\")\n",
        "print(f\"Final rows: {len(df_final)}\")\n",
        "print(f\"Rows removed: {initial_rows - len(df_final)}\")\n",
        "print(f\"Retention rate: {len(df_final)/initial_rows*100:.1f}%\")\n",
        "\n",
        "print(f\"\\n=== DATA QUALITY METRICS ===\")\n",
        "print(f\"\\nFacility IDs: {df_final['facility_id'].notna().sum()} / {len(df_final)} ({df_final['facility_id'].notna().sum()/len(df_final)*100:.1f}%)\")\n",
        "print(f\"Facility Names: {df_final['facility_name'].notna().sum()} / {len(df_final)} ({df_final['facility_name'].notna().sum()/len(df_final)*100:.1f}%)\")\n",
        "print(f\"Facility Types: {df_final['facility_type'].notna().sum()} / {len(df_final)} ({df_final['facility_type'].notna().sum()/len(df_final)*100:.1f}%)\")\n",
        "print(f\"Capacity: {df_final['capacity_numeric'].notna().sum()} / {len(df_final)} ({df_final['capacity_numeric'].notna().sum()/len(df_final)*100:.1f}%)\")\n",
        "print(f\"Region: {df_final['region'].notna().sum()} / {len(df_final)} ({df_final['region'].notna().sum()/len(df_final)*100:.1f}%)\")\n",
        "print(f\"Licence Date: {df_final['licence_issue_date'].notna().sum()} / {len(df_final)} ({df_final['licence_issue_date'].notna().sum()/len(df_final)*100:.1f}%)\")\n",
        "print(f\"Inspection Date: {df_final['inspection_date'].notna().sum()} / {len(df_final)} ({df_final['inspection_date'].notna().sum()/len(df_final)*100:.1f}%)\")\n",
        "print(f\"Coordinates: {df_final['latitude'].notna().sum()} / {len(df_final)} ({df_final['latitude'].notna().sum()/len(df_final)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n=== DISTRIBUTIONS ===\")\n",
        "print(f\"\\nFacility Types:\\n{df_final['facility_type'].value_counts()}\")\n",
        "print(f\"\\nRegions:\\n{df_final['region'].value_counts()}\")\n",
        "print(f\"\\nCapacity Units:\\n{df_final['capacity_unit'].value_counts()}\")\n",
        "print(f\"\\nDate Anomalies: {df_final['date_anomaly'].sum()}\")\n",
        "\n",
        "print(f\"\\n=== STATISTICS ===\")\n",
        "if df_final['capacity_numeric'].notna().sum() > 0:\n",
        "    print(f\"\\nCapacity statistics:\")\n",
        "    print(df_final['capacity_numeric'].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Save Cleaned Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to CSV\n",
        "df_final.to_csv('cleaned_health_registry.csv', index=False)\n",
        "print(\"Cleaned dataset saved to 'cleaned_health_registry.csv'\")\n",
        "print(f\"Final shape: {df_final.shape}\")\n",
        "\n",
        "# Display sample of cleaned data\n",
        "print(\"\\nSample of cleaned data:\")\n",
        "df_final.head(10)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
